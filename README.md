# MLOps Project: Modular Pipeline for ML Lifecycle

This repository contains my first structured MLOps project where I took a traditional Jupyter workflow and turned it into a modular, reproducible, and scalable machine learning pipeline.

---

## ğŸš€ Project Overview

This project takes a dataset through the full ML lifecycle:

* Data ingestion
* Validation
* Transformation
* Model training
* Evaluation
* Deployment via Flask API

The entire pipeline is built using object-oriented programming, YAML configurations, and clearly separated modules.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ artifacts/                  # Stores all output from pipeline stages
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ data_validation/
â”‚   â”œâ”€â”€ data_transformation/
â”‚   â”œâ”€â”€ model_trainer/
â”‚   â””â”€â”€ model_evaluation/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # Central configuration for all pipeline stages
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ running_logs.log        # Central log file
â”‚
â”œâ”€â”€ research/                   # Jupyter notebooks used for exploration
â”‚
â”œâ”€â”€ src/                        # Source code for all components
â”‚   â”œâ”€â”€ components/             # Execution logic for each stage
â”‚   â”œâ”€â”€ config/                 # ConfigurationManager class
â”‚   â”œâ”€â”€ constants/              # Shared constants
â”‚   â”œâ”€â”€ entity/                 # Dataclass config schemas
â”‚   â”œâ”€â”€ pipeline/               # How components run together
â”‚   â””â”€â”€ utils/                  # Common helper functions
â”‚
â”œâ”€â”€ schema.yaml                 # Defines expected data schema
â”œâ”€â”€ params.yaml                 # Model training hyperparameters
â”œâ”€â”€ main.py                     # Runs the full pipeline
â”œâ”€â”€ app.py                      # Flask API to serve model
â””â”€â”€ template.py                 # Script to auto-generate project folders
```

---

## âš™ï¸ How to Run

### 1. Clone the Repo

```bash
git clone https://github.com/NiranJosh101/mlops_basic_project.git
cd mlops_basic_project
```

### 2. Set Up Environment

```bash
pip install -r requirements.txt
```

### 3. Run the Pipeline

```bash
python main.py
```

### 4. Serve Model via API

```bash
python app.py
```

Then open your browser or API tool and POST to `http://localhost:5000/predict`

---

## ğŸ§  What I Learned

* How to modularize code using OOP principles
* The value of separating config from logic (using YAML)
* How to think more like an engineer than just a data scientist
* How to lay a foundation for reusable, production-like workflows

Big thanks to [Krish Naik](https://www.youtube.com/@KrishNaik) for the helpful content and repo inspiration!

---

## ğŸ“Œ TODO

* [ ] Add unit tests
* [ ] Integrate MLflow for experiment tracking
* [ ] Dockerize the API
* [ ] Add CI/CD pipeline with GitHub Actions

---

## ğŸ“¬ Contact

Feel free to reach out if you have questions, suggestions, or want to collaborate!

\[Josh Niran] â€“ \[[hello@niranjosh.pro]]

---

### â­ If this project helped you or inspired your own structure, feel free to star it!
